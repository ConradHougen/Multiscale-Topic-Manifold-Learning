{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "from numpy import linalg as LA\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from wordcloud import WordCloud\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "\n",
    "import random\n",
    "from datetime import datetime, timedelta\n",
    "from multiprocessing import Pool\n",
    "\n",
    "import itertools\n",
    "from itertools import groupby\n",
    "from operator import itemgetter\n",
    "import os\n",
    "import re\n",
    "import sys\n",
    "import pickle\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from mpl_toolkits.mplot3d import axes3d\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from scipy.sparse import csr_matrix, find\n",
    "import sparse_dot_topn.sparse_dot_topn as ct\n",
    "from sparse_dot_topn import awesome_cossim_topn\n",
    "from gensim.models.ldamodel import LdaModel\n",
    "from collections import Counter\n",
    "\n",
    "\n",
    "from AToMS.atoms_gdltm_utils import *\n",
    "from AToMS.author_disambiguation import *\n",
    "\n",
    "np.set_printoptions(suppress=True)\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xml.etree.ElementTree as Xet\n",
    "\n",
    "import gensim\n",
    "from gensim.utils import simple_preprocess\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "import gensim.corpora as corpora\n",
    "\n",
    "import pyLDAvis.gensim_models\n",
    "import pyLDAvis\n",
    "\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dset = 'arxiv'\n",
    "dsub = 'ml_special' # format here is 'oai_cs_CL' for cs.CL category, or 'oai_multi', 'ml_special' for hand-selected subjects, or just 'oai_all' for no filter\n",
    "MAX_AUTHORS_PER_DOC = 20\n",
    "\n",
    "VOCAB_HIGH_FREQ_THRESH_FRAC = 0.995 # If more than X fraction of documents contain a word, remove the word\n",
    "VOCAB_LOW_FREQ_THRESH = 1 # If no more than X docs contain a word, remove it\n",
    "\n",
    "# For filtering terms by relevancy\n",
    "lambda_param = 0.6\n",
    "top_n = 2000\n",
    "num_topics = 50\n",
    "\n",
    "src_dir = '../data/'+dset+'/original/'\n",
    "int_dir = '../data/'+dset+'/intermediate/'+dsub+'/'\n",
    "fig_dir = '../experiments/'+dset+'/'+dsub+'/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Create directories if they don't exist\n",
    "if not os.path.exists(src_dir):\n",
    "    os.makedirs(src_dir)\n",
    "if not os.path.exists(int_dir):\n",
    "    os.makedirs(int_dir)\n",
    "if not os.path.exists(fig_dir):\n",
    "    os.makedirs(fig_dir)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieve all .pkl documents from the src_dir\n",
    "src_json = src_dir + 'arxiv-metadata-oai-snapshot.json'\n",
    "\n",
    "if dsub == 'oai_multi':\n",
    "    #cat_lbl = ['cs.CV', 'hep-th', 'q-bio', 'econ', 'math.AG', 'astro-ph']\n",
    "    #cat_lbl = ['cs.DS', 'physics.flu-dyn', 'math.GT', 'cond-mat.quant-gas']\n",
    "    cat_lbl = [\n",
    "        'cs.CR',  # Cryptography and Security\n",
    "        'math.DG',  # Differential Geometry\n",
    "        'math.PR',  # Probability\n",
    "        'physics.flu-dyn',  # Fluid Dynamics\n",
    "    ]\n",
    "elif dsub == 'ml_special':\n",
    "    cat_lbl = [\n",
    "        'stat.AP',\n",
    "        'stat.CO',\n",
    "        'stat.ME',\n",
    "        'stat.OT',\n",
    "        'stat.TH',\n",
    "        'cs.LG',\n",
    "    ]\n",
    "elif dsub != 'oai_all':\n",
    "    cat_lbl = [dsub.split('_')[1] + '.' + dsub.split('_')[2]]\n",
    "else: # dsub == 'oai_all'\n",
    "     cat_lbl = [\n",
    "        'astro-ph.CO', 'astro-ph.EP', 'astro-ph.GA', 'astro-ph.HE', 'astro-ph.IM', 'astro-ph.SR',\n",
    "        'cond-mat.dis-nn', 'cond-mat.mes-hall', 'cond-mat.mtrl-sci', 'cond-mat.other', 'cond-mat.quant-gas',\n",
    "        'cond-mat.soft', 'cond-mat.stat-mech', 'cond-mat.str-el', 'cond-mat.supr-con',\n",
    "        'cs.AI', 'cs.AR', 'cs.CC', 'cs.CE', 'cs.CG', 'cs.CL', 'cs.CR', 'cs.CV', 'cs.CY', 'cs.DB', 'cs.DC',\n",
    "        'cs.DL', 'cs.DM', 'cs.DS', 'cs.ET', 'cs.FL', 'cs.GL', 'cs.GR', 'cs.GT', 'cs.HC', 'cs.IR', 'cs.IT',\n",
    "        'cs.LG', 'cs.LO', 'cs.MA', 'cs.MM', 'cs.MS', 'cs.NA', 'cs.NE', 'cs.NI', 'cs.OH', 'cs.OS', 'cs.PF',\n",
    "        'cs.PL', 'cs.RO', 'cs.SC', 'cs.SD', 'cs.SE', 'cs.SI', 'cs.SY',\n",
    "        'econ.EM', 'econ.GN', 'econ.TH',\n",
    "        'eess.AS', 'eess.IV', 'eess.SP', 'eess.SY',\n",
    "        'gr-qc',\n",
    "        'hep-ex', 'hep-lat', 'hep-ph', 'hep-th',\n",
    "        'math.AC', 'math.AG', 'math.AP', 'math.AT', 'math.CA', 'math.CO', 'math.CT', 'math.CV', 'math.DG',\n",
    "        'math.DS', 'math.FA', 'math.GM', 'math.GN', 'math.GR', 'math.GT', 'math.HO', 'math.IT', 'math.KT',\n",
    "        'math.LO', 'math.MG', 'math.MP', 'math.NA', 'math.NT', 'math.OA', 'math.OC', 'math.PR', 'math.QA',\n",
    "        'math.RA', 'math.RT', 'math.SG', 'math.SP', 'math.ST',\n",
    "        'nlin.AO', 'nlin.CD', 'nlin.CG', 'nlin.PS', 'nlin.SI',\n",
    "        'nucl-ex', 'nucl-th',\n",
    "        'physics.acc-ph', 'physics.ao-ph', 'physics.app-ph', 'physics.atm-clus', 'physics.atom-ph',\n",
    "        'physics.bio-ph', 'physics.chem-ph', 'physics.class-ph', 'physics.comp-ph', 'physics.data-an',\n",
    "        'physics.ed-ph', 'physics.flu-dyn', 'physics.gen-ph', 'physics.geo-ph', 'physics.hist-ph',\n",
    "        'physics.ins-det', 'physics.med-ph', 'physics.optics', 'physics.plasm-ph', 'physics.pop-ph',\n",
    "        'physics.soc-ph', 'physics.space-ph',\n",
    "        'q-bio.BM', 'q-bio.CB', 'q-bio.GN', 'q-bio.MN', 'q-bio.NC', 'q-bio.OT', 'q-bio.PE', 'q-bio.QM',\n",
    "        'q-bio.SC', 'q-bio.TO',\n",
    "        'q-fin.CP', 'q-fin.EC', 'q-fin.GN', 'q-fin.MF', 'q-fin.PM', 'q-fin.PR', 'q-fin.RM', 'q-fin.ST',\n",
    "        'q-fin.TR',\n",
    "        'quant-ph',\n",
    "        'stat.AP', 'stat.CO', 'stat.ME', 'stat.ML', 'stat.OT', 'stat.TH'\n",
    "    ]\n",
    "\n",
    "\n",
    "\n",
    "arxiv_data = pd.read_json(src_json, lines=True)\n",
    "arxiv_data['date'] = arxiv_data['update_date']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Filter and collate data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arxiv_data['date'] = arxiv_data['update_date']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sort/filter docs\n",
    "arxiv_data = arxiv_data.sort_values(by='date')\n",
    "\n",
    "# Drop columns that aren't of interest\n",
    "arxiv_data = arxiv_data[['id', 'title', 'categories', 'abstract', 'date', 'authors_parsed']]\n",
    "arxiv_data = arxiv_data.dropna()\n",
    "arxiv_data = arxiv_data.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create authors column and standardize format\n",
    "arxiv_data['authors'] = arxiv_data['authors_parsed'].map(lambda authlist: [x[0]+', '+x[1] for x in authlist])\n",
    "arxiv_data['authors'] = arxiv_data['authors'].map(lambda authlist: [str.upper(str.strip(auth_i)) for auth_i in authlist])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter by category label\n",
    "cat_lbl_mask = arxiv_data['categories'].apply(lambda x: any(f in x for f in cat_lbl))\n",
    "arxiv_data = arxiv_data[cat_lbl_mask]\n",
    "\n",
    "arxiv_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple preprocess removes punctuation and returns lowercase words as list, in place of full strings\n",
    "arxiv_data['text_processed'] = arxiv_data['abstract'].map(lambda x: super_simple_preprocess(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_words = list(arxiv_data['text_processed'])\n",
    "id2word = corpora.Dictionary(data_words) # Create dictionary of words, keyed by word ID\n",
    "vlen1 = len(id2word)\n",
    "print(\"Original vocab length: \" + str(vlen1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with Pool() as pool:\n",
    "    data_words = pool.map(lemmatize_mp, data_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "id2word = corpora.Dictionary(data_words)\n",
    "vlen2 = len(id2word)\n",
    "print(\"Vocab length after lemmatization: \" + str(vlen2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove rare and common words to shrink the vocab (improves topic model)\n",
    "id2docfreq = count_doc_freq_of_words(data_words, id2word)\n",
    "ndocs = len(data_words)\n",
    "sw_extension = ['system', 'word', 'method', 'text', 'approach', 'based', 'case', 'model', 'well']\n",
    "for wordId, docfreq in id2docfreq.items():\n",
    "    if docfreq > VOCAB_HIGH_FREQ_THRESH_FRAC * ndocs:\n",
    "        sw_extension.append(id2word[wordId])\n",
    "    if docfreq <= VOCAB_LOW_FREQ_THRESH:\n",
    "        sw_extension.append(id2word[wordId])\n",
    "print(\"Cutting {} words from vocab due to being extremely rare or common...\".format(len(sw_extension)))\n",
    "\n",
    "# Use multiprocessing to speed up the next section\n",
    "stop_words = stopwords.words('english')\n",
    "stop_words.extend(['from', 'subject', 're', 'edu', 'use', 'sub', 'sup', 'used', 'using'])\n",
    "stop_words.extend(sw_extension)\n",
    "stop_words = set(stop_words)\n",
    "# Create approved words list\n",
    "all_tokens = set(id2word.token2id.keys())\n",
    "approved_tokens = sorted([token for token in all_tokens.difference(stop_words) if len(token) > 2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mp_iterable is an iterable list for the arguments to be passed to the multi-processes\n",
    "mp_iterable = [(data_words[i], approved_tokens) for i in range(len(data_words))]\n",
    "with Pool() as pool:\n",
    "    data_words = pool.starmap(filter_stopwords_by_approval, mp_iterable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "id2word = corpora.Dictionary(data_words)\n",
    "vlen2 = len(id2word)\n",
    "print(\"Vocab length after removing stopwords: \" + str(vlen2))"
   ]
  },
  {
   "cell_type": "raw",
   "source": [
    "# Step 1: Prepare the corpus and train LDA model\n",
    "corpus = [id2word.doc2bow(text) for text in arxiv_data['text_processed']]\n",
    "lda_model = LdaModel(corpus=corpus, num_topics=num_topics, id2word=id2word, passes=1)\n",
    "\n",
    "# Step 2: Get the topic-term matrix\n",
    "topic_term_matrix = lda_model.get_topics()\n",
    "\n",
    "# Convert topic-term matrix into a more accessible format (dictionary of terms and their probabilities)\n",
    "topic_term_dict = {}\n",
    "for topic_id, topic_dist in enumerate(topic_term_matrix):\n",
    "    # Get sorted term indices for each topic\n",
    "    top_term_indices = np.argsort(topic_dist)[::-1]  # Sort terms by their probability in descending order\n",
    "    # Convert term IDs to words and store their probabilities\n",
    "    top_terms = [(id2word[term_id], topic_dist[term_id]) for term_id in top_term_indices]\n",
    "    topic_term_dict[topic_id] = top_terms\n",
    "\n",
    "# Step 3: Calculate overall term frequencies (P(w)) across the corpus\n",
    "all_terms = [term for doc_terms in arxiv_data['text_processed'] for term in doc_terms]\n",
    "term_counts = Counter(all_terms)\n",
    "\n",
    "total_terms_in_corpus = sum(term_counts.values())\n",
    "p_w = {term: count / total_terms_in_corpus for term, count in term_counts.items()}\n",
    "\n",
    "# Step 4: Calculate relevancy scores for each term in each topic\n",
    "relevancy_scores = {}\n",
    "for topic_id, term_probs in topic_term_dict.items():\n",
    "    for term, p_w_given_t in term_probs:  # term_probs is a list, so we iterate directly\n",
    "        # Calculate relevancy score\n",
    "        if term in p_w:\n",
    "            relevancy = lambda_param * p_w_given_t + (1 - lambda_param) * (p_w_given_t / p_w[term])\n",
    "            relevancy_scores[(term, topic_id)] = relevancy\n",
    "\n",
    "# Step 5: Get the top relevant terms for each topic\n",
    "top_terms_by_topic = {}\n",
    "\n",
    "for topic_id in range(num_topics):\n",
    "    # Filter relevancy scores for the current topic\n",
    "    topic_terms = {term: relevancy for (term, t_id), relevancy in relevancy_scores.items() if t_id == topic_id}\n",
    "\n",
    "    # Get the top N terms for the topic\n",
    "    top_terms = sorted(topic_terms, key=topic_terms.get, reverse=True)[:top_n]\n",
    "\n",
    "    # Store top terms for this topic\n",
    "    top_terms_by_topic[topic_id] = top_terms\n",
    "\n",
    "# Step 6: Reduce the vocab to the top terms across all topics\n",
    "reduced_vocabulary = set()\n",
    "for terms in top_terms_by_topic.values():\n",
    "    reduced_vocabulary.update(terms)\n",
    "\n",
    "# Step 7: Create a new filtered dictionary and corpus\n",
    "new_id2word = corpora.Dictionary()  # Create a new dictionary for filtered vocabulary\n",
    "new_id2word.add_documents([[term] for term in reduced_vocabulary])  # Add only terms from reduced vocabulary\n",
    "\n",
    "# Recreate the corpus using the new filtered dictionary\n",
    "filtered_corpus = []\n",
    "for doc in arxiv_data['text_processed']:\n",
    "    # Create BoW for the document using the filtered dictionary\n",
    "    bow = new_id2word.doc2bow([term for term in doc if term in new_id2word.token2id])\n",
    "    filtered_corpus.append(bow)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Step 1: Prepare the corpus and train LDA model\n",
    "corpus = [id2word.doc2bow(text) for text in arxiv_data['text_processed']]\n",
    "lda_model = LdaModel(corpus=corpus, num_topics=num_topics, id2word=id2word, passes=1)\n",
    "\n",
    "# Step 2: Get the topic-term matrix\n",
    "topic_term_matrix = lda_model.get_topics()\n",
    "\n",
    "# Convert topic-term matrix into a more accessible format (dictionary of terms and their probabilities)\n",
    "topic_term_dict = {}\n",
    "for topic_id, topic_dist in enumerate(topic_term_matrix):\n",
    "    # Get sorted term indices for each topic\n",
    "    top_term_indices = np.argsort(topic_dist)[::-1]  # Sort terms by their probability in descending order\n",
    "    # Convert term IDs to words and store their probabilities\n",
    "    top_terms = [(id2word[term_id], topic_dist[term_id]) for term_id in top_term_indices]\n",
    "    topic_term_dict[topic_id] = top_terms\n",
    "\n",
    "# Step 3: Calculate overall term frequencies (P(w)) across the corpus\n",
    "all_terms = [term for doc_terms in arxiv_data['text_processed'] for term in doc_terms]\n",
    "term_counts = Counter(all_terms)\n",
    "\n",
    "total_terms_in_corpus = sum(term_counts.values())\n",
    "p_w = {term: count / total_terms_in_corpus for term, count in term_counts.items()}\n",
    "\n",
    "# Step 4: Calculate relevancy scores using log odds for each term in each topic\n",
    "epsilon = 1e-12  # Small constant to avoid log(0)\n",
    "relevancy_scores = {}\n",
    "\n",
    "for topic_id, term_probs in topic_term_dict.items():\n",
    "    for term, p_w_given_t in term_probs:  # term_probs is a list, so we iterate directly\n",
    "        # Calculate relevancy score using log odds\n",
    "        if term in p_w:\n",
    "            # Smooth probabilities to avoid log(0)\n",
    "            p_w_given_t = max(p_w_given_t, epsilon)\n",
    "            p_w_smoothed = max(p_w[term], epsilon)\n",
    "\n",
    "            # Log odds computation\n",
    "            log_p_w_given_t = np.log(p_w_given_t)\n",
    "            log_p_w = np.log(p_w_smoothed)\n",
    "            log_odds = log_p_w_given_t - log_p_w\n",
    "\n",
    "            # Combine log terms with lambda_param\n",
    "            relevancy = lambda_param * log_p_w_given_t + (1 - lambda_param) * log_odds\n",
    "            relevancy_scores[(term, topic_id)] = relevancy\n",
    "\n",
    "# Step 5: Get the top relevant terms for each topic\n",
    "top_terms_by_topic = {}\n",
    "\n",
    "for topic_id in range(num_topics):\n",
    "    # Filter relevancy scores for the current topic\n",
    "    topic_terms = {term: relevancy for (term, t_id), relevancy in relevancy_scores.items() if t_id == topic_id}\n",
    "\n",
    "    # Get the top N terms for the topic\n",
    "    top_terms = sorted(topic_terms, key=topic_terms.get, reverse=True)[:top_n]\n",
    "\n",
    "    # Store top terms for this topic\n",
    "    top_terms_by_topic[topic_id] = top_terms\n",
    "\n",
    "# Step 6: Reduce the vocab to the top terms across all topics\n",
    "reduced_vocabulary = set()\n",
    "for terms in top_terms_by_topic.values():\n",
    "    reduced_vocabulary.update(terms)\n",
    "\n",
    "# Step 7: Create a new filtered dictionary and corpus\n",
    "new_id2word = corpora.Dictionary()  # Create a new dictionary for filtered vocabulary\n",
    "new_id2word.add_documents([[term] for term in reduced_vocabulary])  # Add only terms from reduced vocabulary\n",
    "\n",
    "# Recreate the corpus using the new filtered dictionary\n",
    "filtered_corpus = []\n",
    "for doc in arxiv_data['text_processed']:\n",
    "    # Create BoW for the document using the filtered dictionary\n",
    "    bow = new_id2word.doc2bow([term for term in doc if term in new_id2word.token2id])\n",
    "    filtered_corpus.append(bow)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "arxiv_data['text_processed'] = arxiv_data['text_processed'].apply(lambda doc: filter_terms(doc, reduced_vocabulary))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Update text_processed to remove any words not in dictionary from the main dataframe\n",
    "#arxiv_data['text_processed'] = data_words\n",
    "# Drop rows with no text after processing\n",
    "arxiv_data = arxiv_data[arxiv_data['text_processed'].apply(lambda x: len(x) > 0)]\n",
    "\n",
    "# Drop null data again in case we cut out an entire row somehow\n",
    "arxiv_data = arxiv_data.dropna()\n",
    "arxiv_data = arxiv_data.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mem_use = arxiv_data.memory_usage(deep=True)\n",
    "print(\"arxiv_data size: {:,} bytes\".format(sum(mem_use)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save in pickle file\n",
    "arxiv_data.to_pickle(int_dir + 'collated_only.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Author disambiguation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create list of all authors\n",
    "all_authors = []\n",
    "author_to_arxivId = []\n",
    "for index, row in arxiv_data.iterrows():\n",
    "\n",
    "    authors = row['authors']\n",
    "\n",
    "    # If there are too many authors, scrap the document as it is not meaningful\n",
    "    if len(authors) > MAX_AUTHORS_PER_DOC:\n",
    "        arxiv_data = arxiv_data.drop(index)\n",
    "\n",
    "    arxiv_id = row['id']\n",
    "    for author in authors:\n",
    "        # Only add the author if their name starts with a letter (to prevent junk)\n",
    "        if re.search(\"^[a-zA-Z]\", author) is not None:\n",
    "            author_to_arxivId.append(arxiv_id)\n",
    "            all_authors.append(author)\n",
    "\n",
    "# sort all_authors alphabetically along with the map to osti IDs\n",
    "all_authors, author_to_arxivId = (list(t) for t in zip(*sorted(zip(all_authors, author_to_arxivId))))\n",
    "\n",
    "# Upper case all author names and use first 20 characters only\n",
    "all_authors = list(map(str.upper, all_authors))\n",
    "all_authors = [name[0:20].strip() for name in all_authors]\n",
    "\n",
    "with open(int_dir + 'all_authors.pkl', 'wb') as f:\n",
    "    pickle.dump(all_authors, f)\n",
    "with open(int_dir + 'author_to_arxivId.pkl', 'wb') as f:\n",
    "    pickle.dump(author_to_arxivId, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reduce list to set of unique author names\n",
    "authors_collapsed = set(all_authors)\n",
    "authors_collapsed = list(authors_collapsed)\n",
    "authors_collapsed.sort()\n",
    "ceil_nauth = len(authors_collapsed)\n",
    "\n",
    "print(\"Number of authors prior to disambiguation = \" + str(ceil_nauth))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dflen = len(arxiv_data)\n",
    "print(\"Number of docs in corpus: \" + str(dflen))\n",
    "\n",
    "arxiv_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_sz_wfe_single = ceil_nauth * vlen1 * 4 / 1000000000\n",
    "max_sz_wfe_double = ceil_nauth * vlen1 * 8 / 1000000000\n",
    "print(\"Approx. size of author wf embeddings with all words (single|double): {:,}GiB|{:,}GiB\".format(max_sz_wfe_single, max_sz_wfe_double))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_sz_wfe_single = ceil_nauth * vlen2 * 4 / 1000000000\n",
    "max_sz_wfe_double = ceil_nauth * vlen2 * 8 / 1000000000\n",
    "print(\"Approx. size of author wf embeddings with reduced vocab (single|double): {:,}GiB|{:,}GiB\".format(max_sz_wfe_single, max_sz_wfe_double))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split author list by first 2 letters of last name\n",
    "authors_split = [list(authnames) for letter, authnames in groupby(authors_collapsed, key=itemgetter(slice(0, 2)))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of different starting characters\n",
    "num_first_letters = len(authors_split)\n",
    "print(\"Number of unique first letters of author names = \" + str(num_first_letters))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run tfidf vectorizer and perform matching on each author sub-list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save all find(match) results from the split author name list\n",
    "fmatch_mat = []\n",
    "\n",
    "for split_idx in range(num_first_letters):\n",
    "    vectorizer = TfidfVectorizer(min_df=1, analyzer=ngrams)\n",
    "    tf_idf_matrix = vectorizer.fit_transform(authors_split[split_idx])\n",
    "\n",
    "    topN = len(authors_split[split_idx])\n",
    "    matches = awesome_cossim_top(tf_idf_matrix, tf_idf_matrix.transpose(), topN, 0.90)\n",
    "\n",
    "    print(split_idx, tf_idf_matrix.shape, matches.nnz, (abs(matches-matches.T)>1e-10).nnz)\n",
    "\n",
    "    tmp = find(matches)\n",
    "    fmatch_mat.append(tmp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "author_to_authorId, authorId_to_author = assign_author_ids(authors_split, fmatch_mat)\n",
    "with open(int_dir + 'author_to_authorId.pkl', 'wb') as f:\n",
    "    pickle.dump(author_to_authorId, f)\n",
    "with open(int_dir + 'authorId_to_author.pkl', 'wb') as f:\n",
    "    pickle.dump(authorId_to_author, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Number of authors after disambiguation: \" + str(len(authorId_to_author)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initiate column in df for authorID\n",
    "arxiv_data['authorID'] = np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for index, row in arxiv_data.iterrows():\n",
    "    if index % 10000 == 0:\n",
    "        print(index)\n",
    "\n",
    "    authors = row[\"authors\"]\n",
    "\n",
    "    # For each author, look up the author ID and create a list\n",
    "    auth_ids_for_row = ''\n",
    "    #auth_names_for_row = ''\n",
    "    for auth in authors:\n",
    "        auth = auth[0:20].strip()\n",
    "        if re.search(\"^[a-zA-Z]\", auth) is not None: # Throwaway all junk\n",
    "            new_auth_id = author_to_authorId[auth]\n",
    "            auth_ids_for_row = auth_ids_for_row + str(new_auth_id) + ','\n",
    "\n",
    "    arxiv_data.loc[index, 'authorID'] = auth_ids_for_row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_words = list(arxiv_data['text_processed'])\n",
    "id2word = corpora.Dictionary(data_words) # Create dictionary of words, keyed by word ID\n",
    "vlen3 = len(id2word)\n",
    "print(\"Final vocab length: \" + str(vlen3))\n",
    "print(\"Final data_words length: \" + str(len(data_words)))\n",
    "if not os.path.exists(int_dir):\n",
    "    os.makedirs(int_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "with open(int_dir + 'data_words.pkl', 'wb') as f:\n",
    "    pickle.dump(data_words, f)\n",
    "\n",
    "with open(int_dir + 'id2word.pkl', 'wb') as f:\n",
    "    pickle.dump(id2word, f)\n",
    "\n",
    "# Save the pkl file with all the added author ID information\n",
    "arxiv_data.to_pickle(int_dir + 'main_df.pkl')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "create_and_save_wordcloud(dset, dsub)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate both the normal multigraphs and the edge-labeled multigraphs\n",
    "# First is for testing, second is for visualization purposes\n",
    "gen_nx_multigraphs_per_year(dset, dsub, overwrite=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate complete hypergraph\n",
    "H = gen_hypergraph(dset, dsub)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hnx.drawing.draw(H)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
