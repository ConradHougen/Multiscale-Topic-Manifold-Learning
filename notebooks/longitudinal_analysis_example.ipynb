{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MSTML Longitudinal Analysis Example\n",
    "\n",
    "This notebook demonstrates how to use the MSTML framework for longitudinal analysis of research topics and collaboration patterns over time.\n",
    "\n",
    "## Overview\n",
    "\n",
    "Longitudinal analysis in MSTML examines how topics and collaboration patterns evolve over time by:\n",
    "\n",
    "1. **Time-Chunked Topic Modeling**: Dividing documents into time periods and modeling topics for each chunk\n",
    "2. **Hierarchical Topic Clustering**: Creating dendrograms of topic relationships across time\n",
    "3. **PHATE Embedding**: Visualizing topic evolution in low-dimensional space\n",
    "4. **Meta-Topic Analysis**: Analyzing higher-level topic clusters and their temporal dynamics\n",
    "5. **Smooth Path Analysis**: Tracking topic trajectories through the embedding space\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "Before running this notebook:\n",
    "1. Run `python setup_mstml.py` to set up the environment\n",
    "2. Place your preprocessed data in `data/clean/` directory\n",
    "3. Ensure your data has temporal information (date column)\n",
    "4. Data should follow the standard MSTML DataFrame format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import os\n",
    "import sys\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "import phate\n",
    "from datetime import datetime, timedelta\n",
    "from scipy.cluster.hierarchy import linkage, fcluster, dendrogram\n",
    "from scipy.spatial.distance import pdist, squareform\n",
    "\n",
    "# Add the source directory to Python path\n",
    "sys.path.append('../source')\n",
    "\n",
    "# Import MSTML components\n",
    "from source import (\n",
    "    MstmlLongitudinalAnalysis, \n",
    "    MstmlParams, \n",
    "    MstmlEmbedType,\n",
    "    GdltmParams\n",
    ")\n",
    "\n",
    "from source.mstml_library import (\n",
    "    get_chunk_to_meta_mapping,\n",
    "    get_meta_topic_distributions,\n",
    "    plot_phate_embedding_with_filtered_chunks,\n",
    "    select_smooth_path,\n",
    "    refine_smooth_path,\n",
    "    rescale_parameter,\n",
    "    plot_wordcloud_for_topic\n",
    ")\n",
    "\n",
    "print(\"✓ Imports successful\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuration\n",
    "\n",
    "Set up the analysis parameters and data paths."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "DATASET_NAME = \"your_dataset\"  # Change this to your dataset name\n",
    "DATA_PATH = f\"../data/clean/{DATASET_NAME}.pkl\"  # Path to your preprocessed data\n",
    "\n",
    "# Temporal analysis parameters\n",
    "CHUNK_SIZE_MONTHS = 12  # Size of each time chunk in months\n",
    "MIN_DOCS_PER_CHUNK = 50  # Minimum documents required per chunk\n",
    "N_TOPICS_PER_CHUNK = 15  # Number of topics per time chunk\n",
    "\n",
    "# Hierarchical clustering parameters\n",
    "CUT_HEIGHT = 0.3  # Dendrogram cut height for meta-topics\n",
    "LINKAGE_METHOD = 'ward'  # Linkage method for hierarchical clustering\n",
    "\n",
    "# PHATE embedding parameters\n",
    "PHATE_PARAMS = {\n",
    "    'n_components': 3,  # 3D embedding for better visualization\n",
    "    'knn': 5,\n",
    "    'decay': 40,\n",
    "    'n_jobs': -1\n",
    "}\n",
    "\n",
    "print(f\"Dataset: {DATASET_NAME}\")\n",
    "print(f\"Chunk size: {CHUNK_SIZE_MONTHS} months\")\n",
    "print(f\"Topics per chunk: {N_TOPICS_PER_CHUNK}\")\n",
    "print(f\"Cut height: {CUT_HEIGHT}\")\n",
    "print(f\"PHATE components: {PHATE_PARAMS['n_components']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Loading and Temporal Preparation\n",
    "\n",
    "Load the data and prepare it for temporal analysis by creating time chunks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "if not os.path.exists(DATA_PATH):\n",
    "    print(f\"❌ Data file not found: {DATA_PATH}\")\n",
    "    print(\"Please ensure you have:\")\n",
    "    print(\"1. Run the preprocessing scripts to create clean data\")\n",
    "    print(\"2. Updated the DATASET_NAME variable above\")\n",
    "    print(\"3. Placed your data in the correct directory\")\n",
    "    raise FileNotFoundError(f\"Data file not found: {DATA_PATH}\")\n",
    "\n",
    "# Load the DataFrame\n",
    "df = pd.read_pickle(DATA_PATH)\n",
    "\n",
    "print(f\"✓ Loaded dataset with {len(df)} documents\")\n",
    "print(f\"Date range: {df['date'].min()} to {df['date'].max()}\")\n",
    "\n",
    "# Validate temporal data\n",
    "if 'date' not in df.columns:\n",
    "    raise ValueError(\"Dataset must contain a 'date' column for longitudinal analysis\")\n",
    "\n",
    "# Convert date column to datetime if needed\n",
    "if not pd.api.types.is_datetime64_any_dtype(df['date']):\n",
    "    df['date'] = pd.to_datetime(df['date'])\n",
    "\n",
    "# Sort by date\n",
    "df = df.sort_values('date').reset_index(drop=True)\n",
    "\n",
    "print(\"✓ Data validation and sorting completed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Time Chunk Creation\n",
    "\n",
    "Divide the dataset into temporal chunks for longitudinal analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create time chunks\n",
    "def create_time_chunks(df, chunk_size_months, min_docs_per_chunk):\n",
    "    \"\"\"Create time chunks from the dataset.\"\"\"\n",
    "    chunks = []\n",
    "    chunk_info = []\n",
    "    \n",
    "    start_date = df['date'].min()\n",
    "    end_date = df['date'].max()\n",
    "    \n",
    "    current_date = start_date\n",
    "    chunk_id = 0\n",
    "    \n",
    "    while current_date < end_date:\n",
    "        # Calculate chunk end date\n",
    "        chunk_end = current_date + pd.DateOffset(months=chunk_size_months)\n",
    "        \n",
    "        # Filter documents in this time period\n",
    "        chunk_mask = (df['date'] >= current_date) & (df['date'] < chunk_end)\n",
    "        chunk_df = df[chunk_mask].copy()\n",
    "        \n",
    "        # Only include chunks with sufficient documents\n",
    "        if len(chunk_df) >= min_docs_per_chunk:\n",
    "            chunks.append(chunk_df)\n",
    "            chunk_info.append({\n",
    "                'chunk_id': chunk_id,\n",
    "                'start_date': current_date,\n",
    "                'end_date': chunk_end,\n",
    "                'n_docs': len(chunk_df),\n",
    "                'date_range': f\"{current_date.strftime('%Y-%m')} to {chunk_end.strftime('%Y-%m')}\"\n",
    "            })\n",
    "            chunk_id += 1\n",
    "        else:\n",
    "            print(f\"Skipping chunk {current_date.strftime('%Y-%m')} (only {len(chunk_df)} documents)\")\n",
    "        \n",
    "        current_date = chunk_end\n",
    "    \n",
    "    return chunks, chunk_info\n",
    "\n",
    "# Create chunks\n",
    "time_chunks, chunk_info = create_time_chunks(df, CHUNK_SIZE_MONTHS, MIN_DOCS_PER_CHUNK)\n",
    "\n",
    "print(f\"✓ Created {len(time_chunks)} time chunks\")\n",
    "print(\"\\nChunk Information:\")\n",
    "for info in chunk_info:\n",
    "    print(f\"Chunk {info['chunk_id']}: {info['date_range']} ({info['n_docs']} docs)\")\n",
    "\n",
    "# Visualize chunk distribution\n",
    "chunk_sizes = [info['n_docs'] for info in chunk_info]\n",
    "chunk_labels = [f\"Chunk {info['chunk_id']}\\n{info['start_date'].strftime('%Y-%m')}\" for info in chunk_info]\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.bar(range(len(chunk_sizes)), chunk_sizes)\n",
    "plt.xlabel('Time Chunk')\n",
    "plt.ylabel('Number of Documents')\n",
    "plt.title('Document Distribution Across Time Chunks')\n",
    "plt.xticks(range(len(chunk_labels)), chunk_labels, rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Topic Modeling for Each Time Chunk\n",
    "\n",
    "Train topic models for each time chunk to capture temporal topic evolution.\n",
    "\n",
    "**Note**: This is a simplified example. In practice, you would use the full GDLTM pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is a placeholder for the actual topic modeling pipeline\n",
    "# In practice, you would use GDLTM to train topic models for each chunk\n",
    "\n",
    "print(\"⚠️  Topic modeling pipeline placeholder\")\n",
    "print(\"In a complete implementation, this would include:\")\n",
    "print(\"1. LDA topic model training for each time chunk\")\n",
    "print(\"2. Topic-word distribution extraction\")\n",
    "print(\"3. Document-topic distribution computation\")\n",
    "print(\"4. Hellinger distance calculation between topics\")\n",
    "\n",
    "# For demonstration, create mock topic data\n",
    "np.random.seed(42)\n",
    "\n",
    "# Mock topic-word distributions for each chunk\n",
    "vocab_size = 1000\n",
    "chunk_topic_distributions = []\n",
    "chunk_topic_names = []\n",
    "\n",
    "for i, chunk in enumerate(time_chunks):\n",
    "    # Create random topic-word distributions\n",
    "    topic_word_dists = []\n",
    "    topic_names = []\n",
    "    \n",
    "    for j in range(N_TOPICS_PER_CHUNK):\n",
    "        # Random topic distribution\n",
    "        topic_dist = np.random.dirichlet(np.ones(vocab_size))\n",
    "        topic_word_dists.append(topic_dist)\n",
    "        topic_names.append(f\"Chunk{i}_Topic{j}\")\n",
    "    \n",
    "    chunk_topic_distributions.append(np.array(topic_word_dists))\n",
    "    chunk_topic_names.append(topic_names)\n",
    "\n",
    "print(f\"✓ Created mock topic distributions for {len(time_chunks)} chunks\")\n",
    "print(f\"Each chunk has {N_TOPICS_PER_CHUNK} topics with {vocab_size} vocabulary terms\")\n",
    "print(\"⚠️  Replace this with actual GDLTM topic modeling results\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hierarchical Topic Clustering\n",
    "\n",
    "Create hierarchical clusters of topics across all time chunks using Hellinger distances."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine all topic distributions\n",
    "all_topic_distributions = np.vstack(chunk_topic_distributions)\n",
    "all_topic_names = [name for chunk_names in chunk_topic_names for name in chunk_names]\n",
    "\n",
    "print(f\"Total topics across all chunks: {len(all_topic_distributions)}\")\n",
    "\n",
    "# Calculate Hellinger distances between topics\n",
    "def hellinger_distance(p, q):\n",
    "    \"\"\"Calculate Hellinger distance between two probability distributions.\"\"\"\n",
    "    return np.sqrt(0.5 * np.sum((np.sqrt(p) - np.sqrt(q)) ** 2))\n",
    "\n",
    "# Compute pairwise Hellinger distances\n",
    "n_topics = len(all_topic_distributions)\n",
    "distance_matrix = np.zeros((n_topics, n_topics))\n",
    "\n",
    "print(\"Computing Hellinger distance matrix...\")\n",
    "for i in range(n_topics):\n",
    "    for j in range(i + 1, n_topics):\n",
    "        dist = hellinger_distance(all_topic_distributions[i], all_topic_distributions[j])\n",
    "        distance_matrix[i, j] = dist\n",
    "        distance_matrix[j, i] = dist\n",
    "\n",
    "print(\"✓ Distance matrix computed\")\n",
    "\n",
    "# Perform hierarchical clustering\n",
    "condensed_distances = squareform(distance_matrix)\n",
    "linkage_matrix = linkage(condensed_distances, method=LINKAGE_METHOD)\n",
    "\n",
    "print(f\"✓ Hierarchical clustering completed using {LINKAGE_METHOD} linkage\")\n",
    "\n",
    "# Plot dendrogram\n",
    "plt.figure(figsize=(15, 8))\n",
    "dendrogram(\n",
    "    linkage_matrix,\n",
    "    labels=all_topic_names,\n",
    "    leaf_rotation=90,\n",
    "    leaf_font_size=8\n",
    ")\n",
    "plt.title('Hierarchical Clustering of Topics Across Time Chunks')\n",
    "plt.xlabel('Topics')\n",
    "plt.ylabel('Distance')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Meta-Topic Analysis\n",
    "\n",
    "Create meta-topics by cutting the dendrogram at a specified height and analyze their temporal evolution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get meta-topic mapping\n",
    "chunk_to_meta, n_meta_topics = get_chunk_to_meta_mapping(linkage_matrix, CUT_HEIGHT)\n",
    "\n",
    "print(f\"✓ Created {n_meta_topics} meta-topics at cut height {CUT_HEIGHT}\")\n",
    "\n",
    "# Create mapping from original topic indices to meta-topics\n",
    "topic_to_meta = {}\n",
    "for i, meta_topic in chunk_to_meta.items():\n",
    "    topic_to_meta[i] = meta_topic\n",
    "\n",
    "# Analyze meta-topic composition\n",
    "meta_topic_composition = {}\n",
    "for meta_id in range(n_meta_topics):\n",
    "    topics_in_meta = [i for i, meta in topic_to_meta.items() if meta == meta_id]\n",
    "    \n",
    "    # Group by chunk\n",
    "    chunk_counts = {}\n",
    "    for topic_idx in topics_in_meta:\n",
    "        chunk_id = topic_idx // N_TOPICS_PER_CHUNK\n",
    "        if chunk_id not in chunk_counts:\n",
    "            chunk_counts[chunk_id] = 0\n",
    "        chunk_counts[chunk_id] += 1\n",
    "    \n",
    "    meta_topic_composition[meta_id] = {\n",
    "        'total_topics': len(topics_in_meta),\n",
    "        'chunk_distribution': chunk_counts,\n",
    "        'topic_indices': topics_in_meta\n",
    "    }\n",
    "\n",
    "print(\"\\nMeta-Topic Composition:\")\n",
    "for meta_id, comp in meta_topic_composition.items():\n",
    "    print(f\"Meta-Topic {meta_id}: {comp['total_topics']} topics across {len(comp['chunk_distribution'])} chunks\")\n",
    "    chunk_dist_str = \", \".join([f\"Chunk {c}: {count}\" for c, count in comp['chunk_distribution'].items()])\n",
    "    print(f\"  Distribution: {chunk_dist_str}\")\n",
    "\n",
    "# Visualize meta-topic temporal distribution\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for meta_id in range(min(4, n_meta_topics)):\n",
    "    comp = meta_topic_composition[meta_id]\n",
    "    chunks = list(comp['chunk_distribution'].keys())\n",
    "    counts = list(comp['chunk_distribution'].values())\n",
    "    \n",
    "    axes[meta_id].bar(chunks, counts)\n",
    "    axes[meta_id].set_title(f'Meta-Topic {meta_id} Distribution')\n",
    "    axes[meta_id].set_xlabel('Time Chunk')\n",
    "    axes[meta_id].set_ylabel('Number of Topics')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PHATE Embedding\n",
    "\n",
    "Create low-dimensional embeddings of topics using PHATE to visualize topic evolution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create PHATE embedding\n",
    "print(\"Computing PHATE embedding...\")\n",
    "phate_op = phate.PHATE(**PHATE_PARAMS)\n",
    "phate_embedding = phate_op.fit_transform(all_topic_distributions)\n",
    "\n",
    "print(f\"✓ PHATE embedding completed: {phate_embedding.shape}\")\n",
    "\n",
    "# Create color mapping for chunks\n",
    "chunk_colors = plt.cm.viridis(np.linspace(0, 1, len(time_chunks)))\n",
    "topic_colors = []\n",
    "topic_chunk_labels = []\n",
    "\n",
    "for chunk_id in range(len(time_chunks)):\n",
    "    for _ in range(N_TOPICS_PER_CHUNK):\n",
    "        topic_colors.append(chunk_colors[chunk_id])\n",
    "        topic_chunk_labels.append(chunk_id)\n",
    "\n",
    "# 3D visualization\n",
    "if PHATE_PARAMS['n_components'] == 3:\n",
    "    fig = plt.figure(figsize=(12, 9))\n",
    "    ax = fig.add_subplot(111, projection='3d')\n",
    "    \n",
    "    scatter = ax.scatter(\n",
    "        phate_embedding[:, 0], \n",
    "        phate_embedding[:, 1], \n",
    "        phate_embedding[:, 2],\n",
    "        c=topic_chunk_labels, \n",
    "        cmap='viridis', \n",
    "        s=50, \n",
    "        alpha=0.7\n",
    "    )\n",
    "    \n",
    "    ax.set_xlabel('PHATE 1')\n",
    "    ax.set_ylabel('PHATE 2')\n",
    "    ax.set_zlabel('PHATE 3')\n",
    "    ax.set_title('PHATE Embedding of Topics Across Time Chunks')\n",
    "    \n",
    "    # Add colorbar\n",
    "    cbar = plt.colorbar(scatter, ax=ax, shrink=0.5, aspect=20)\n",
    "    cbar.set_label('Time Chunk')\n",
    "    \n",
    "    plt.show()\n",
    "\n",
    "# 2D projections\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "\n",
    "# PHATE 1 vs PHATE 2\n",
    "scatter1 = axes[0].scatter(\n",
    "    phate_embedding[:, 0], phate_embedding[:, 1], \n",
    "    c=topic_chunk_labels, cmap='viridis', s=50, alpha=0.7\n",
    ")\n",
    "axes[0].set_xlabel('PHATE 1')\n",
    "axes[0].set_ylabel('PHATE 2')\n",
    "axes[0].set_title('PHATE 1 vs PHATE 2')\n",
    "\n",
    "# PHATE 1 vs PHATE 3\n",
    "if PHATE_PARAMS['n_components'] >= 3:\n",
    "    scatter2 = axes[1].scatter(\n",
    "        phate_embedding[:, 0], phate_embedding[:, 2], \n",
    "        c=topic_chunk_labels, cmap='viridis', s=50, alpha=0.7\n",
    "    )\n",
    "    axes[1].set_xlabel('PHATE 1')\n",
    "    axes[1].set_ylabel('PHATE 3')\n",
    "    axes[1].set_title('PHATE 1 vs PHATE 3')\n",
    "    \n",
    "    # PHATE 2 vs PHATE 3\n",
    "    scatter3 = axes[2].scatter(\n",
    "        phate_embedding[:, 1], phate_embedding[:, 2], \n",
    "        c=topic_chunk_labels, cmap='viridis', s=50, alpha=0.7\n",
    "    )\n",
    "    axes[2].set_xlabel('PHATE 2')\n",
    "    axes[2].set_ylabel('PHATE 3')\n",
    "    axes[2].set_title('PHATE 2 vs PHATE 3')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Smooth Path Analysis\n",
    "\n",
    "Analyze smooth paths through the PHATE embedding to understand topic evolution trajectories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select smooth paths through the embedding\n",
    "print(\"Computing smooth paths through PHATE embedding...\")\n",
    "\n",
    "# Path along PHATE 1 dimension\n",
    "path_phate1 = select_smooth_path(\n",
    "    phate_embedding,\n",
    "    primary_dim=0,  # PHATE 1\n",
    "    secondary_dim=1,  # PHATE 2\n",
    "    tertiary_dim=2 if PHATE_PARAMS['n_components'] >= 3 else 1,  # PHATE 3 or PHATE 2\n",
    "    num_points=10,\n",
    "    secondary_method='max',\n",
    "    tertiary_method='max'\n",
    ")\n",
    "\n",
    "# Path along PHATE 2 dimension\n",
    "path_phate2 = select_smooth_path(\n",
    "    phate_embedding,\n",
    "    primary_dim=1,  # PHATE 2\n",
    "    secondary_dim=0,  # PHATE 1\n",
    "    tertiary_dim=2 if PHATE_PARAMS['n_components'] >= 3 else 0,  # PHATE 3 or PHATE 1\n",
    "    num_points=10,\n",
    "    secondary_method='max',\n",
    "    tertiary_method='max'\n",
    ")\n",
    "\n",
    "print(f\"✓ Computed paths with {len(path_phate1)} and {len(path_phate2)} points\")\n",
    "\n",
    "# Visualize paths\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "# Path 1 visualization\n",
    "axes[0].scatter(\n",
    "    phate_embedding[:, 0], phate_embedding[:, 1], \n",
    "    c=topic_chunk_labels, cmap='viridis', s=30, alpha=0.5\n",
    ")\n",
    "path1_points = phate_embedding[path_phate1]\n",
    "axes[0].plot(path1_points[:, 0], path1_points[:, 1], 'r-', linewidth=2, alpha=0.8)\n",
    "axes[0].scatter(path1_points[:, 0], path1_points[:, 1], c='red', s=100, marker='o', edgecolors='black')\n",
    "axes[0].set_xlabel('PHATE 1')\n",
    "axes[0].set_ylabel('PHATE 2')\n",
    "axes[0].set_title('Smooth Path Along PHATE 1 Dimension')\n",
    "\n",
    "# Path 2 visualization\n",
    "axes[1].scatter(\n",
    "    phate_embedding[:, 0], phate_embedding[:, 1], \n",
    "    c=topic_chunk_labels, cmap='viridis', s=30, alpha=0.5\n",
    ")\n",
    "path2_points = phate_embedding[path_phate2]\n",
    "axes[1].plot(path2_points[:, 0], path2_points[:, 1], 'b-', linewidth=2, alpha=0.8)\n",
    "axes[1].scatter(path2_points[:, 0], path2_points[:, 1], c='blue', s=100, marker='s', edgecolors='black')\n",
    "axes[1].set_xlabel('PHATE 1')\n",
    "axes[1].set_ylabel('PHATE 2')\n",
    "axes[1].set_title('Smooth Path Along PHATE 2 Dimension')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Analyze topics along paths\n",
    "print(\"\\nTopics along Path 1 (PHATE 1 dimension):\")\n",
    "for i, topic_idx in enumerate(path_phate1):\n",
    "    topic_name = all_topic_names[topic_idx]\n",
    "    chunk_id = topic_chunk_labels[topic_idx]\n",
    "    meta_topic = topic_to_meta.get(topic_idx, 'Unknown')\n",
    "    print(f\"  {i+1}. {topic_name} (Chunk {chunk_id}, Meta-Topic {meta_topic})\")\n",
    "\n",
    "print(\"\\nTopics along Path 2 (PHATE 2 dimension):\")\n",
    "for i, topic_idx in enumerate(path_phate2):\n",
    "    topic_name = all_topic_names[topic_idx]\n",
    "    chunk_id = topic_chunk_labels[topic_idx]\n",
    "    meta_topic = topic_to_meta.get(topic_idx, 'Unknown')\n",
    "    print(f\"  {i+1}. {topic_name} (Chunk {chunk_id}, Meta-Topic {meta_topic})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Temporal Meta-Topic Evolution\n",
    "\n",
    "Analyze how meta-topics evolve over time and their prevalence in different periods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create temporal evolution matrix\n",
    "evolution_matrix = np.zeros((n_meta_topics, len(time_chunks)))\n",
    "\n",
    "for topic_idx, meta_topic in topic_to_meta.items():\n",
    "    chunk_id = topic_idx // N_TOPICS_PER_CHUNK\n",
    "    if chunk_id < len(time_chunks):  # Safety check\n",
    "        evolution_matrix[meta_topic, chunk_id] += 1\n",
    "\n",
    "# Normalize by chunk size to get relative prevalence\n",
    "chunk_sizes = np.array([N_TOPICS_PER_CHUNK] * len(time_chunks))\n",
    "evolution_matrix_normalized = evolution_matrix / chunk_sizes[np.newaxis, :]\n",
    "\n",
    "# Plot evolution heatmap\n",
    "plt.figure(figsize=(12, 8))\n",
    "sns.heatmap(\n",
    "    evolution_matrix_normalized,\n",
    "    annot=True,\n",
    "    fmt='.2f',\n",
    "    cmap='viridis',\n",
    "    xticklabels=[f'Chunk {i}\\n{info[\"start_date\"].strftime(\"%Y-%m\")}' for i, info in enumerate(chunk_info)],\n",
    "    yticklabels=[f'Meta-Topic {i}' for i in range(n_meta_topics)]\n",
    ")\n",
    "plt.title('Meta-Topic Evolution Over Time (Normalized Prevalence)')\n",
    "plt.xlabel('Time Chunks')\n",
    "plt.ylabel('Meta-Topics')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Plot individual meta-topic trajectories\n",
    "plt.figure(figsize=(12, 6))\n",
    "chunk_dates = [info['start_date'] for info in chunk_info]\n",
    "\n",
    "for meta_id in range(n_meta_topics):\n",
    "    plt.plot(\n",
    "        chunk_dates, \n",
    "        evolution_matrix_normalized[meta_id, :], \n",
    "        marker='o', \n",
    "        label=f'Meta-Topic {meta_id}',\n",
    "        linewidth=2\n",
    "    )\n",
    "\n",
    "plt.xlabel('Time')\n",
    "plt.ylabel('Normalized Prevalence')\n",
    "plt.title('Meta-Topic Prevalence Over Time')\n",
    "plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Identify trending meta-topics\n",
    "print(\"\\nMeta-Topic Trends:\")\n",
    "for meta_id in range(n_meta_topics):\n",
    "    prevalence = evolution_matrix_normalized[meta_id, :]\n",
    "    trend = np.polyfit(range(len(prevalence)), prevalence, 1)[0]  # Linear trend\n",
    "    \n",
    "    if trend > 0.01:\n",
    "        trend_desc = \"📈 Increasing\"\n",
    "    elif trend < -0.01:\n",
    "        trend_desc = \"📉 Decreasing\"\n",
    "    else:\n",
    "        trend_desc = \"➡️ Stable\"\n",
    "    \n",
    "    max_prevalence = np.max(prevalence)\n",
    "    peak_chunk = np.argmax(prevalence)\n",
    "    \n",
    "    print(f\"Meta-Topic {meta_id}: {trend_desc} (slope: {trend:.4f})\")\n",
    "    print(f\"  Peak: {max_prevalence:.3f} in Chunk {peak_chunk} ({chunk_info[peak_chunk]['start_date'].strftime('%Y-%m')})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results Summary and Export\n",
    "\n",
    "Summarize the longitudinal analysis results and export them for further use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create comprehensive results summary\n",
    "results_summary = {\n",
    "    'dataset': DATASET_NAME,\n",
    "    'analysis_type': 'longitudinal',\n",
    "    'temporal_parameters': {\n",
    "        'chunk_size_months': CHUNK_SIZE_MONTHS,\n",
    "        'min_docs_per_chunk': MIN_DOCS_PER_CHUNK,\n",
    "        'n_chunks_created': len(time_chunks),\n",
    "        'date_range': {\n",
    "            'start': df['date'].min().isoformat(),\n",
    "            'end': df['date'].max().isoformat()\n",
    "        }\n",
    "    },\n",
    "    'topic_modeling': {\n",
    "        'n_topics_per_chunk': N_TOPICS_PER_CHUNK,\n",
    "        'total_topics': len(all_topic_distributions),\n",
    "        'vocab_size': vocab_size\n",
    "    },\n",
    "    'hierarchical_clustering': {\n",
    "        'linkage_method': LINKAGE_METHOD,\n",
    "        'cut_height': CUT_HEIGHT,\n",
    "        'n_meta_topics': n_meta_topics\n",
    "    },\n",
    "    'phate_embedding': PHATE_PARAMS,\n",
    "    'meta_topic_composition': meta_topic_composition\n",
    "}\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"LONGITUDINAL ANALYSIS SUMMARY\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "print(f\"Dataset: {DATASET_NAME}\")\n",
    "print(f\"Time period: {df['date'].min().strftime('%Y-%m')} to {df['date'].max().strftime('%Y-%m')}\")\n",
    "print(f\"Number of time chunks: {len(time_chunks)}\")\n",
    "print(f\"Topics per chunk: {N_TOPICS_PER_CHUNK}\")\n",
    "print(f\"Total topics analyzed: {len(all_topic_distributions)}\")\n",
    "print(f\"Meta-topics identified: {n_meta_topics}\")\n",
    "print(f\"PHATE embedding dimensions: {PHATE_PARAMS['n_components']}\")\n",
    "\n",
    "print(\"\\nChunk Distribution:\")\n",
    "for info in chunk_info:\n",
    "    print(f\"  {info['date_range']}: {info['n_docs']} documents\")\n",
    "\n",
    "print(\"\\nMeta-Topic Summary:\")\n",
    "for meta_id, comp in meta_topic_composition.items():\n",
    "    n_chunks_present = len(comp['chunk_distribution'])\n",
    "    total_topics = comp['total_topics']\n",
    "    print(f\"  Meta-Topic {meta_id}: {total_topics} topics across {n_chunks_present} chunks\")\n",
    "\n",
    "# Export results\n",
    "results_dir = Path(\"../results\")\n",
    "results_dir.mkdir(exist_ok=True)\n",
    "\n",
    "# Export PHATE embedding\n",
    "phate_df = pd.DataFrame(\n",
    "    phate_embedding, \n",
    "    columns=[f'PHATE_{i+1}' for i in range(phate_embedding.shape[1])]\n",
    ")\n",
    "phate_df['topic_name'] = all_topic_names\n",
    "phate_df['chunk_id'] = topic_chunk_labels\n",
    "phate_df['meta_topic'] = [topic_to_meta.get(i, -1) for i in range(len(all_topic_names))]\n",
    "phate_df.to_csv(results_dir / f\"{DATASET_NAME}_phate_embedding.csv\", index=False)\n",
    "\n",
    "# Export meta-topic evolution\n",
    "evolution_df = pd.DataFrame(\n",
    "    evolution_matrix_normalized,\n",
    "    columns=[f'Chunk_{i}' for i in range(len(time_chunks))],\n",
    "    index=[f'MetaTopic_{i}' for i in range(n_meta_topics)]\n",
    ")\n",
    "evolution_df.to_csv(results_dir / f\"{DATASET_NAME}_meta_topic_evolution.csv\")\n",
    "\n",
    "# Export summary\n",
    "import json\n",
    "with open(results_dir / f\"{DATASET_NAME}_longitudinal_summary.json\", 'w') as f:\n",
    "    json.dump(results_summary, f, indent=2, default=str)\n",
    "\n",
    "print(f\"\\n✓ Results exported to {results_dir}\")\n",
    "print(\"Files created:\")\n",
    "print(f\"  - {DATASET_NAME}_phate_embedding.csv\")\n",
    "print(f\"  - {DATASET_NAME}_meta_topic_evolution.csv\")\n",
    "print(f\"  - {DATASET_NAME}_longitudinal_summary.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next Steps\n",
    "\n",
    "This notebook provides a comprehensive framework for longitudinal analysis using MSTML. To extend the analysis:\n",
    "\n",
    "1. **Implement Full GDLTM Pipeline**: Replace mock data with actual topic modeling using the complete GDLTM framework\n",
    "2. **Author-Topic Analysis**: Extend to analyze how author expertise evolves over time\n",
    "3. **Network Evolution**: Analyze how collaboration networks change across time periods\n",
    "4. **Predictive Modeling**: Use temporal patterns to predict future topic trends\n",
    "5. **Interactive Visualization**: Create interactive plots for exploring topic evolution\n",
    "6. **Statistical Testing**: Add significance tests for trend analysis\n",
    "7. **Domain-Specific Customization**: Adapt the analysis for specific research domains\n",
    "\n",
    "## References\n",
    "\n",
    "- GDLTM: Geometry-Driven Longitudinal Topic Model (Harvard Data Science Review, 2021)\n",
    "- PHATE: Potential of Heat-diffusion for Affinity-based Transition Embedding\n",
    "- Original AToMS research framework"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",\n",
   "language": "python",\n   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}